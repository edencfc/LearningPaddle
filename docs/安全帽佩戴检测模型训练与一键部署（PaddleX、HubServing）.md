# é¡¹ç›®ç®€ä»‹
å®‰å…¨å¸½ä½©æˆ´æ£€æµ‹æ˜¯è®¡ç®—æœºè§†è§‰åœ¨å·¥ä¸šå®‰å…¨é¢†åŸŸåº”ç”¨çš„å…¸å‹åœºæ™¯ï¼Œæœ¬æ–‡ä½¿ç”¨PaddleXè¿›è¡Œyolov3_darknet53è¿ç§»å­¦ä¹ è®­ç»ƒï¼Œå¹¶æä¾›äº†PaddleXå®æ—¶è§†é¢‘æµé¢„æµ‹éƒ¨ç½²å’ŒPaddleHub-ServingæœåŠ¡åŒ–éƒ¨ç½²ä¸¤ç§æ–¹å¼ã€‚åªéœ€æ•°å°æ—¶ï¼Œå³å¯å®Œæˆä¸€ä¸ªæ•ˆæœè‰¯å¥½çš„å®‰å…¨å¸½ä½©æˆ´æ£€æµ‹æ¨¡å‹å…¨æµç¨‹éƒ¨ç½²ã€‚

# ç¯å¢ƒå‡†å¤‡
## å®‰è£…å·¥å…·åº“


```python
!pip install ipywidgets
```

## è§£å‹æ•°æ®é›†


```python
!mkdir MyDataset
```


```python
!unzip data/data50329/HelmetDetection.zip -d ./MyDataset
```

## åˆ‡åˆ†æ•°æ®é›†
### æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨æœ€æ–°PaddleXçš„developåˆ†æ”¯ï¼ˆä¸æ¨èï¼‰
è¯¥åšæ³•æ­¥éª¤å¦‚ä¸‹ï¼š
1. æ‹‰å–æ¨¡å‹åº“
```bash
# æ‹‰å–PaddleXæ¨¡å‹åº“
!git clone https://gitee.com/paddlepaddle/PaddleX.git
```
2. ä¿®æ”¹PaddleX/paddlex/tools/dataset_split/voc_split.pyæºä»£ç ï¼ˆæ³¨æ„ï¼šå®‰è£…PaddleXå‰å¿…é¡»å…ˆä¿®æ”¹æºä»£ç ï¼‰
å…¶å®å°±æ˜¯å°†VOCæ•°æ®é›†é»˜è®¤çš„`JPEGImages`å’Œ`Annotations`ä¿®æ”¹ä¸ºä¸æ•°æ®é›†å¯¹åº”çš„`images`å’Œ`annotations`ï¼Œä¹‹æ‰€ä»¥æ”¹æºä»£ç è€Œä¸æ˜¯ç›®å½•åï¼ŒåŸå› åœ¨äºæ ‡æ³¨çš„xmlæ–‡ä»¶éƒ½å†™æ˜äº†å›¾ç‰‡å­˜æ”¾è·¯å¾„ä¸º`images`ï¼Œæƒè¡¡ä¹‹ä¸‹ä¿®æ”¹æºä»£ç æ”¹åŠ¨æ›´å°
```python
import os.path as osp
import random
import xml.etree.ElementTree as ET
from .utils import list_files, is_pic, replace_ext
import paddlex.utils.logging as logging


def split_voc_dataset(dataset_dir, val_percent, test_percent, save_dir):
    if not osp.exists(osp.join(dataset_dir, "images")):
        logging.error("\'images\' is not found in {}!".format(dataset_dir))
    if not osp.exists(osp.join(dataset_dir, "annotations")):
        logging.error("\'annotations\' is not found in {}!".format(
            dataset_dir))

    all_image_files = list_files(osp.join(dataset_dir, "images"))

    image_anno_list = list()
    label_list = list()
    for image_file in all_image_files:
        if not is_pic(image_file):
            continue
        anno_name = replace_ext(image_file, "xml")
        if osp.exists(osp.join(dataset_dir, "annotations", anno_name)):
            image_anno_list.append([image_file, anno_name])
            try:
                tree = ET.parse(
                    osp.join(dataset_dir, "annotations", anno_name))
            except:
                raise Exception("æ–‡ä»¶{}ä¸æ˜¯ä¸€ä¸ªè‰¯æ„çš„xmlæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶".format(
                    osp.join(dataset_dir, "annotations", anno_name)))
            objs = tree.findall("object")
            for i, obj in enumerate(objs):
                cname = obj.find('name').text
                if not cname in label_list:
                    label_list.append(cname)
        else:
            logging.error("The annotation file {} doesn't exist!".format(
                anno_name))

    random.shuffle(image_anno_list)
    image_num = len(image_anno_list)
    val_num = int(image_num * val_percent)
    test_num = int(image_num * test_percent)
    train_num = image_num - val_num - test_num

    train_image_anno_list = image_anno_list[:train_num]
    val_image_anno_list = image_anno_list[train_num:train_num + val_num]
    test_image_anno_list = image_anno_list[train_num + val_num:]

    with open(
            osp.join(save_dir, 'train_list.txt'), mode='w',
            encoding='utf-8') as f:
        for x in train_image_anno_list:
            file = osp.join("images", x[0])
            label = osp.join("annotations", x[1])
            f.write('{} {}\n'.format(file, label))
    with open(
            osp.join(save_dir, 'val_list.txt'), mode='w',
            encoding='utf-8') as f:
        for x in val_image_anno_list:
            file = osp.join("images", x[0])
            label = osp.join("annotations", x[1])
            f.write('{} {}\n'.format(file, label))
    if len(test_image_anno_list):
        with open(
                osp.join(save_dir, 'test_list.txt'), mode='w',
                encoding='utf-8') as f:
            for x in test_image_anno_list:
                file = osp.join("images", x[0])
                label = osp.join("annotations", x[1])
                f.write('{} {}\n'.format(file, label))
    with open(
            osp.join(save_dir, 'labels.txt'), mode='w', encoding='utf-8') as f:
        for l in sorted(label_list):
            f.write('{}\n'.format(l))

    return train_num, val_num, test_num
```
3. å®‰è£…PaddleXçš„developåˆ†æ”¯
```bash
# å®‰è£…ä¾èµ–åº“
!pip uninstall paddlehub -y
!pip install colorama -i http://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com
!pip install shapely>=1.7.0 -i http://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com
!pip install paddleslim==1.0.1 -i http://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com
!pip install paddlehub>=1.6.2 -i http://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com
# å®‰è£…PaddleXçš„developåˆ†æ”¯
!cd PaddleX && git checkout develop && python setup.py install 
```

å®‰è£…PaddleXçš„developåˆ†æ”¯åï¼Œå¯ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œåˆ‡åˆ†æ•°æ®é›†ï¼Œæ•ˆæœå¦‚ä¸‹ï¼š


```python
# !paddlex --split_dataset --format VOC --dataset_dir MyDataset --val_value 0.2 --test_value 0.1
```

    Dataset Split Done.[0m
    [0mTrain samples: 3500[0m
    [0mEval samples: 1000[0m
    [0mTest samples: 500[0m
    [0mSplit files saved in MyDataset[0m
    [0m[0m[0m

### æ–¹æ¡ˆäºŒï¼šå‚è€ƒdevelopåˆ†æ”¯çš„voc_split.pyé‡å†™æ•°æ®é›†åˆ‡åˆ†ä»£ç 
è¯¥åšæ³•æ­¥éª¤å¦‚ä¸‹ï¼š
1. ä»pipå®‰è£…PaddleX
2. å°†`voc_split.py`importçš„æ–¹æ³•ä¸€ä¸€æ‰¾å‡ºï¼Œåœ¨Notebookä¸­è¿è¡Œ
3. ä¿®æ”¹voc_split.pyåˆ‡åˆ†æ•°æ®é›†æ—¶å¯¹åº”çš„æ–‡ä»¶ç›®å½•å


```python
# pipå®‰è£…PaddleX
!pip install paddlex
```


```python
# PaddleX/paddlex/tools/dataset_split/utils.py

# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import os.path as osp
from PIL import Image
import numpy as np
import json


class MyEncoder(json.JSONEncoder):
    # è°ƒæ•´jsonæ–‡ä»¶å­˜å‚¨å½¢å¼
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return super(MyEncoder, self).default(obj)


def list_files(dirname):
    """ åˆ—å‡ºç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼ˆåŒ…æ‹¬æ‰€å±çš„ä¸€çº§å­ç›®å½•ä¸‹æ–‡ä»¶ï¼‰

    Args:
        dirname: ç›®å½•è·¯å¾„
    """

    def filter_file(f):
        if f.startswith('.'):
            return True
        return False

    all_files = list()
    dirs = list()
    for f in os.listdir(dirname):
        if filter_file(f):
            continue
        if osp.isdir(osp.join(dirname, f)):
            dirs.append(f)
        else:
            all_files.append(f)
    for d in dirs:
        for f in os.listdir(osp.join(dirname, d)):
            if filter_file(f):
                continue
            if osp.isdir(osp.join(dirname, d, f)):
                continue
            all_files.append(osp.join(d, f))
    return all_files


def is_pic(filename):
    """ åˆ¤æ–­æ–‡ä»¶æ˜¯å¦ä¸ºå›¾ç‰‡æ ¼å¼

    Args:
        filename: æ–‡ä»¶è·¯å¾„
    """
    suffixes = {'JPEG', 'jpeg', 'JPG', 'jpg', 'BMP', 'bmp', 'PNG', 'png'}
    suffix = filename.strip().split('.')[-1]
    if suffix not in suffixes:
        return False
    return True


def replace_ext(filename, new_ext):
    """ æ›¿æ¢æ–‡ä»¶åç¼€

    Args:
        filename: æ–‡ä»¶è·¯å¾„
        new_ext: éœ€è¦æ›¿æ¢çš„æ–°çš„åç¼€
    """
    items = filename.split(".")
    items[-1] = new_ext
    new_filename = ".".join(items)
    return new_filename


def read_seg_ann(pngfile):
    """ è§£æè¯­ä¹‰åˆ†å‰²çš„æ ‡æ³¨pngå›¾ç‰‡

    Args:
        pngfile: åŒ…å«æ ‡æ³¨ä¿¡æ¯çš„pngå›¾ç‰‡è·¯å¾„
    """
    grt = np.asarray(Image.open(pngfile))
    labels = list(np.unique(grt))
    if 255 in labels:
        labels.remove(255)
    return labels

```


```python
# PaddleX/paddlex/utils/logging.py

# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import time
import os
import sys
import colorama
from colorama import init
import paddlex

init(autoreset=True)
levels = {0: 'ERROR', 1: 'WARNING', 2: 'INFO', 3: 'DEBUG'}


def log(level=2, message="", use_color=False):
    current_time = time.time()
    time_array = time.localtime(current_time)
    current_time = time.strftime("%Y-%m-%d %H:%M:%S", time_array)
    if paddlex.log_level >= level:
        if use_color:
            print("\033[1;31;40m{} [{}]\t{}\033[0m".format(
                current_time, levels[level], message).encode("utf-8").decode(
                    "latin1"))
        else:
            print("{} [{}]\t{}".format(current_time, levels[level], message)
                  .encode("utf-8").decode("latin1"))
        sys.stdout.flush()


def debug(message="", use_color=False):
    log(level=3, message=message, use_color=use_color)


def info(message="", use_color=False):
    log(level=2, message=message, use_color=use_color)


def warning(message="", use_color=True):
    log(level=1, message=message, use_color=use_color)


def error(message="", use_color=True, exit=True):
    log(level=0, message=message, use_color=use_color)
    if exit:
        sys.exit(-1)

```


```python
# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os.path as osp
import random
import xml.etree.ElementTree as ET


def split_voc_dataset(dataset_dir, val_percent, test_percent, save_dir):
    # æ³¨æ„å›¾ç‰‡ç›®å½•å’Œæ ‡æ³¨ç›®å½•åå·²å…¨éƒ¨ä¿®æ”¹
    if not osp.exists(osp.join(dataset_dir, "images")):
        logging.error("\'images\' is not found in {}!".format(dataset_dir))
    if not osp.exists(osp.join(dataset_dir, "annotations")):
        logging.error("\'annotations\' is not found in {}!".format(
            dataset_dir))

    all_image_files = list_files(osp.join(dataset_dir, "images"))

    image_anno_list = list()
    label_list = list()
    for image_file in all_image_files:
        if not is_pic(image_file):
            continue
        anno_name = replace_ext(image_file, "xml")
        if osp.exists(osp.join(dataset_dir, "annotations", anno_name)):
            image_anno_list.append([image_file, anno_name])
            try:
                tree = ET.parse(
                    osp.join(dataset_dir, "annotations", anno_name))
            except:
                raise Exception("æ–‡ä»¶{}ä¸æ˜¯ä¸€ä¸ªè‰¯æ„çš„xmlæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶".format(
                    osp.join(dataset_dir, "annotations", anno_name)))
            objs = tree.findall("object")
            for i, obj in enumerate(objs):
                cname = obj.find('name').text
                if not cname in label_list:
                    label_list.append(cname)
        else:
            logging.error("The annotation file {} doesn't exist!".format(
                anno_name))

    random.shuffle(image_anno_list)
    image_num = len(image_anno_list)
    val_num = int(image_num * val_percent)
    test_num = int(image_num * test_percent)
    train_num = image_num - val_num - test_num

    train_image_anno_list = image_anno_list[:train_num]
    val_image_anno_list = image_anno_list[train_num:train_num + val_num]
    test_image_anno_list = image_anno_list[train_num + val_num:]

    with open(
            osp.join(save_dir, 'train_list.txt'), mode='w',
            encoding='utf-8') as f:
        for x in train_image_anno_list:
            file = osp.join("images", x[0])
            label = osp.join("annotations", x[1])
            f.write('{} {}\n'.format(file, label))
    with open(
            osp.join(save_dir, 'val_list.txt'), mode='w',
            encoding='utf-8') as f:
        for x in val_image_anno_list:
            file = osp.join("images", x[0])
            label = osp.join("annotations", x[1])
            f.write('{} {}\n'.format(file, label))
    if len(test_image_anno_list):
        with open(
                osp.join(save_dir, 'test_list.txt'), mode='w',
                encoding='utf-8') as f:
            for x in test_image_anno_list:
                file = osp.join("images", x[0])
                label = osp.join("annotations", x[1])
                f.write('{} {}\n'.format(file, label))
    with open(
            osp.join(save_dir, 'labels.txt'), mode='w', encoding='utf-8') as f:
        for l in sorted(label_list):
            f.write('{}\n'.format(l))

    return train_num, val_num, test_num


if __name__ == "__main__":
    # åˆ‡åˆ†æ•°æ®é›†
    split_voc_dataset('MyDataset', 0.2, 0.1, 'MyDataset')

```

# è®­ç»ƒyolov3_darknet53è¿ç§»å­¦ä¹ æ¨¡å‹
è¿™é‡Œç›´æ¥ä½¿ç”¨[å®˜æ–¹æ–‡æ¡£yolov3_darknet53è®­ç»ƒä»£ç ](https://github.com/PaddlePaddle/PaddleX/blob/develop/tutorials/train/object_detection/yolov3_darknet53.py)ï¼Œæ³¨é‡Šéå¸¸è¯¦ç»†ï¼Œå‡ ä¹æ²¡æœ‰æ”¹åŠ¨ã€‚

VisualDLè®­ç»ƒè¿‡ç¨‹è¯·æŸ¥çœ‹`output/yolov3_darknet53/vdl_log`ç›®å½•ã€‚

1. lossä¸‹é™è¶‹åŠ¿

![file](https://ai-studio-static-online.cdn.bcebos.com/19d0b3f2ef7d4a8f9d4707ce1270b50e7b69d9afbd324c949e06ee7808fbe1c2)

2. å­¦ä¹ ç‡å˜åŒ–

![file](https://ai-studio-static-online.cdn.bcebos.com/da67f4062831452392f50c8b754d340d4fb8b31cc6da47b89094b43d9432de45)

3. éªŒè¯é›†ä¸Šbbox_mapå˜åŒ–

![file](https://ai-studio-static-online.cdn.bcebos.com/9fdde2c046394465a973a2dac23aa22377f914a7cf584637a572bf82bf1b166e)



```python
# ç¯å¢ƒå˜é‡é…ç½®ï¼Œç”¨äºæ§åˆ¶æ˜¯å¦ä½¿ç”¨GPU
# è¯´æ˜æ–‡æ¡£ï¼šhttps://paddlex.readthedocs.io/zh_CN/develop/appendix/parameters.html#gpu
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

from paddlex.det import transforms
import paddlex as pdx

# å®šä¹‰è®­ç»ƒå’ŒéªŒè¯æ—¶çš„transforms
# APIè¯´æ˜ https://paddlex.readthedocs.io/zh_CN/develop/apis/transforms/det_transforms.html
train_transforms = transforms.Compose([
    transforms.MixupImage(mixup_epoch=250), transforms.RandomDistort(),
    transforms.RandomExpand(), transforms.RandomCrop(), transforms.Resize(
        target_size=608, interp='RANDOM'), transforms.RandomHorizontalFlip(),
    transforms.Normalize()
])

eval_transforms = transforms.Compose([
    transforms.Resize(
        target_size=608, interp='CUBIC'), transforms.Normalize()
])

# å®šä¹‰è®­ç»ƒå’ŒéªŒè¯æ‰€ç”¨çš„æ•°æ®é›†
# APIè¯´æ˜ï¼šhttps://paddlex.readthedocs.io/zh_CN/develop/apis/datasets.html#paddlex-datasets-vocdetection
train_dataset = pdx.datasets.VOCDetection(
    data_dir='MyDataset',
    file_list='MyDataset/train_list.txt',
    label_list='MyDataset/labels.txt',
    transforms=train_transforms,
    shuffle=True)
eval_dataset = pdx.datasets.VOCDetection(
    data_dir='MyDataset',
    file_list='MyDataset/val_list.txt',
    label_list='MyDataset/labels.txt',
    transforms=eval_transforms)

# åˆå§‹åŒ–æ¨¡å‹ï¼Œå¹¶è¿›è¡Œè®­ç»ƒ
# å¯ä½¿ç”¨VisualDLæŸ¥çœ‹è®­ç»ƒæŒ‡æ ‡ï¼Œå‚è€ƒhttps://paddlex.readthedocs.io/zh_CN/develop/train/visualdl.html
num_classes = len(train_dataset.labels)

# APIè¯´æ˜: https://paddlex.readthedocs.io/zh_CN/develop/apis/models/detection.html#paddlex-det-yolov3
model = pdx.det.YOLOv3(num_classes=num_classes, backbone='DarkNet53')

# APIè¯´æ˜: https://paddlex.readthedocs.io/zh_CN/develop/apis/models/detection.html#train
# å„å‚æ•°ä»‹ç»ä¸è°ƒæ•´è¯´æ˜ï¼šhttps://paddlex.readthedocs.io/zh_CN/develop/appendix/parameters.html
model.train(
    num_epochs=270,
    train_dataset=train_dataset,
    train_batch_size=8,
    eval_dataset=eval_dataset,
    learning_rate=0.000125,
    lr_decay_epochs=[210, 240],
    save_dir='output/yolov3_darknet53',
    use_vdl=True)
```

# Pythonéƒ¨ç½²
PaddleXå·²ç»é›†æˆäº†åŸºäºPythonçš„é«˜æ€§èƒ½é¢„æµ‹æ¥å£ï¼Œä¸‹é¢æ¼”ç¤ºå•å¼ å›¾ç‰‡å’Œè§†é¢‘æµçš„é¢„æµ‹æ•ˆæœã€‚
## å¯¼å‡ºé¢„æµ‹æ¨¡å‹


```python
!paddlex --export_inference --model_dir=./output/yolov3_darknet53/epoch_220 --save_dir=./inference_model
```

    W0830 11:54:15.850441 10972 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
    W0830 11:54:15.854879 10972 device_context.cc:260] device: 0, cuDNN Version: 7.3.
    2020-08-30 11:54:20 [INFO]	Model[YOLOv3] loaded.
    2020-08-30 11:54:21 [INFO]	Model for inference deploy saved in ./inference_model.


### å•å¼ å›¾ç‰‡é¢„æµ‹
é€‰æ‹©ä¸€å¼ æµ‹è¯•é›†ä¸­çš„å›¾ç‰‡ï¼ŒæŸ¥çœ‹é¢„æµ‹æ•ˆæœ


```python
import paddlex as pdx
predictor = pdx.deploy.Predictor('./inference_model')
result = predictor.predict(image='MyDataset/images/hard_hat_workers1457.png')
```


```python
%matplotlib inline
import matplotlib.pyplot as plt # plt ç”¨äºæ˜¾ç¤ºå›¾ç‰‡
import numpy as np
import cv2

# è¯»å–åŸå§‹å›¾ç‰‡
origin_pic = cv2.imread('MyDataset/images/hard_hat_workers1457.png')
origin_pic = cv2.cvtColor(origin_pic, cv2.COLOR_BGR2RGB)
plt.imshow(origin_pic)
plt.axis('off') # ä¸æ˜¾ç¤ºåæ ‡è½´
plt.show()
```


![png](../imgs/output_19_0.png)



```python
result
```




    [{'category_id': 1,
      'bbox': [131.40003967285156,
       2.1436729431152344,
       114.73101806640625,
       107.99861526489258],
      'score': 0.9960896372795105,
      'category': 'helmet'}]



### è§†é¢‘æµé¢„æµ‹
åœ¨AI Studioä¸­ä¸èƒ½æ¼”ç¤ºå®æ—¶æ•ˆæœï¼Œå› æ­¤é‡‡ç”¨å°†é¢„æµ‹å›¾ç‰‡ä¿å­˜å†åˆæˆè§†é¢‘çš„å½¢å¼ã€‚åŒæ ·éœ€è¦é‡å†™`paddlex.det.visualize()`æ–¹æ³•


```python
??paddlex.det.visualize
```


```python
def draw_bbox_mask(image, results, threshold=0.5):
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib as mpl
    import matplotlib.figure as mplfigure
    import matplotlib.colors as mplc
    from matplotlib.backends.backend_agg import FigureCanvasAgg

    # refer to  https://github.com/facebookresearch/detectron2/blob/master/detectron2/utils/visualizer.py
    def _change_color_brightness(color, brightness_factor):
        assert brightness_factor >= -1.0 and brightness_factor <= 1.0
        color = mplc.to_rgb(color)
        polygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))
        modified_lightness = polygon_color[1] + (brightness_factor *
                                                 polygon_color[1])
        modified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness
        modified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness
        modified_color = colorsys.hls_to_rgb(
            polygon_color[0], modified_lightness, polygon_color[2])
        return modified_color

    _SMALL_OBJECT_AREA_THRESH = 1000
    # setup figure
    width, height = image.shape[1], image.shape[0]
    scale = 1
    fig = mplfigure.Figure(frameon=False)
    dpi = fig.get_dpi()
    fig.set_size_inches(
        (width * scale + 1e-2) / dpi,
        (height * scale + 1e-2) / dpi, )
    canvas = FigureCanvasAgg(fig)
    ax = fig.add_axes([0.0, 0.0, 1.0, 1.0])
    ax.axis("off")
    ax.set_xlim(0.0, width)
    ax.set_ylim(height)
    default_font_size = max(np.sqrt(height * width) // 90, 10 // scale)
    linewidth = max(default_font_size / 4, 1)

    labels = list()
    for dt in np.array(results):
        if dt['category'] not in labels:
            labels.append(dt['category'])
    color_map = get_color_map_list(256)

    keep_results = []
    areas = []
    for dt in np.array(results):
        cname, bbox, score = dt['category'], dt['bbox'], dt['score']
        if score < threshold:
            continue
        keep_results.append(dt)
        areas.append(bbox[2] * bbox[3])
    areas = np.asarray(areas)
    sorted_idxs = np.argsort(-areas).tolist()
    keep_results = [keep_results[k]
                    for k in sorted_idxs] if len(keep_results) > 0 else []

    for dt in np.array(keep_results):
        cname, bbox, score = dt['category'], dt['bbox'], dt['score']
        xmin, ymin, w, h = bbox
        xmax = xmin + w
        ymax = ymin + h

        color = tuple(color_map[labels.index(cname) + 2])
        color = [c / 255. for c in color]
        # draw bbox
        ax.add_patch(
            mpl.patches.Rectangle(
                (xmin, ymin),
                w,
                h,
                fill=False,
                edgecolor=color,
                linewidth=linewidth * scale,
                alpha=0.8,
                linestyle="-", ))

        # draw mask
        if 'mask' in dt:
            mask = dt['mask']
            mask = np.ascontiguousarray(mask)
            res = cv2.findContours(
                mask.astype("uint8"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)
            hierarchy = res[-1]
            alpha = 0.5
            if hierarchy is not None:
                has_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0
                res = res[-2]
                res = [x.flatten() for x in res]
                res = [x for x in res if len(x) >= 6]
                for segment in res:
                    segment = segment.reshape(-1, 2)
                    edge_color = mplc.to_rgb(color) + (1, )
                    polygon = mpl.patches.Polygon(
                        segment,
                        fill=True,
                        facecolor=mplc.to_rgb(color) + (alpha, ),
                        edgecolor=edge_color,
                        linewidth=max(default_font_size // 15 * scale, 1), )
                    ax.add_patch(polygon)

        # draw label
        text_pos = (xmin, ymin)
        horiz_align = "left"
        instance_area = w * h
        if (instance_area < _SMALL_OBJECT_AREA_THRESH * scale or
                h < 40 * scale):
            if ymin >= height - 5:
                text_pos = (xmin, ymin)
            else:
                text_pos = (xmin, ymax)
        height_ratio = h / np.sqrt(height * width)
        font_size = (np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2,
                             2) * 0.5 * default_font_size)
        text = "{} {:.2f}".format(cname, score)
        color = np.maximum(list(mplc.to_rgb(color)), 0.2)
        color[np.argmax(color)] = max(0.8, np.max(color))
        color = _change_color_brightness(color, brightness_factor=0.7)
        ax.text(
            text_pos[0],
            text_pos[1],
            text,
            size=font_size * scale,
            family="sans-serif",
            bbox={
                "facecolor": "black",
                "alpha": 0.8,
                "pad": 0.7,
                "edgecolor": "none"
            },
            verticalalignment="top",
            horizontalalignment=horiz_align,
            color=color,
            zorder=10,
            rotation=0, )

    s, (width, height) = canvas.print_to_buffer()
    buffer = np.frombuffer(s, dtype="uint8")

    img_rgba = buffer.reshape(height, width, 4)
    rgb, alpha = np.split(img_rgba, [3], axis=2)

    try:
        import numexpr as ne
        visualized_image = ne.evaluate(
            "image * (1 - alpha / 255.0) + rgb * (alpha / 255.0)")
    except ImportError:
        alpha = alpha.astype("float32") / 255.0
        visualized_image = image * (1 - alpha) + rgb * alpha

    visualized_image = visualized_image.astype("uint8")

    return visualized_image
```


```python
def get_color_map_list(num_classes):
    """ Returns the color map for visualizing the segmentation mask,
        which can support arbitrary number of classes.
    Args:
        num_classes: Number of classes
    Returns:
        The color map
    """
    color_map = num_classes * [0, 0, 0]
    for i in range(0, num_classes):
        j = 0
        lab = i
        while lab:
            color_map[i * 3] |= (((lab >> 0) & 1) << (7 - j))
            color_map[i * 3 + 1] |= (((lab >> 1) & 1) << (7 - j))
            color_map[i * 3 + 2] |= (((lab >> 2) & 1) << (7 - j))
            j += 1
            lab >>= 3
    color_map = [color_map[i:i + 3] for i in range(0, len(color_map), 3)]
    return color_map
```


```python
def visualize(image, result, threshold=0.5, img_num=0,save_dir='./'):
    """
        Visualize bbox and mask results
    """

    if isinstance(image, np.ndarray):
        image_name = str(img_num) + '.jpg'
    else:
        image_name = os.path.split(image)[-1]
        image = cv2.imread(image)

    image = draw_bbox_mask(image, result, threshold=threshold)
    if save_dir is not None:
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        out_path = os.path.join(save_dir, '{}'.format(image_name))
        cv2.imwrite(out_path, image)
        print('The visualized result is saved as {}'.format(out_path))
    else:
        return image
```


```python
import time
import os
import sys
import colorama
from colorama import init
import paddlex

init(autoreset=True)
levels = {0: 'ERROR', 1: 'WARNING', 2: 'INFO', 3: 'DEBUG'}


def log(level=2, message="", use_color=False):
    current_time = time.time()
    time_array = time.localtime(current_time)
    current_time = time.strftime("%Y-%m-%d %H:%M:%S", time_array)
    if paddlex.log_level >= level:
        if use_color:
            print("\033[1;31;40m{} [{}]\t{}\033[0m".format(
                current_time, levels[level], message).encode("utf-8").decode(
                    "latin1"))
        else:
            print("{} [{}]\t{}".format(current_time, levels[level], message)
                  .encode("utf-8").decode("latin1"))
        sys.stdout.flush()


def debug(message="", use_color=False):
    log(level=3, message=message, use_color=use_color)


def info(message="", use_color=False):
    log(level=2, message=message, use_color=use_color)


def warning(message="", use_color=True):
    log(level=1, message=message, use_color=use_color)


def error(message="", use_color=True, exit=True):
    log(level=0, message=message, use_color=use_color)
    if exit:
        sys.exit(-1)

```


```python
import cv2
import paddlex as pdx
import numpy as np
import colorsys
import os

predictor = pdx.deploy.Predictor('./inference_model')
cap = cv2.VideoCapture('./hatdet.mp4')
i = 1
while cap.isOpened():
    ret, frame = cap.read()
    if ret:
        result = predictor.predict(frame)
        print(i)
        vis_img = visualize(frame, result, threshold=0.4, img_num=i, save_dir='hatdet')
        i += 1
        # æœ¬åœ°ç¯å¢ƒå¯ä»¥å®æ—¶æŸ¥çœ‹å®‰å…¨å¸½æ£€æµ‹æ•ˆæœ
        # cv2.imshow('hatdet', vis_img)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    else:
        break
cap.release()
```


```python
# å°†å›¾ç‰‡åˆæˆè§†é¢‘
!ffmpeg -f image2 -i ./hatdet/%d.jpg -vcodec libx264 -r 30 video.mp4
```

    ffmpeg version 2.8.15-0ubuntu0.16.04.1 Copyright (c) 2000-2018 the FFmpeg developers
      built with gcc 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.10) 20160609
      configuration: --prefix=/usr --extra-version=0ubuntu0.16.04.1 --build-suffix=-ffmpeg --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --cc=cc --cxx=g++ --enable-gpl --enable-shared --disable-stripping --disable-decoder=libopenjpeg --disable-decoder=libschroedinger --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzvbi --enable-openal --enable-opengl --enable-x11grab --enable-libdc1394 --enable-libiec61883 --enable-libzmq --enable-frei0r --enable-libx264 --enable-libopencv
      libavutil      54. 31.100 / 54. 31.100
      libavcodec     56. 60.100 / 56. 60.100
      libavformat    56. 40.101 / 56. 40.101
      libavdevice    56.  4.100 / 56.  4.100
      libavfilter     5. 40.101 /  5. 40.101
      libavresample   2.  1.  0 /  2.  1.  0
      libswscale      3.  1.101 /  3.  1.101
      libswresample   1.  2.101 /  1.  2.101
      libpostproc    53.  3.100 / 53.  3.100
    [0;36m[mjpeg @ 0x6f0720] [0mChangeing bps to 8
    Input #0, image2, from './hatdet/%d.jpg':
      Duration: 00:00:14.96, start: 0.000000, bitrate: N/A
        Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1920x1080 [SAR 1:1 DAR 16:9], 25 fps, 25 tbr, 25 tbn, 25 tbc
    [0;33mNo pixel format specified, yuvj420p for H.264 encoding chosen.
    Use -pix_fmt yuv420p for compatibility with outdated media players.
    [0m[1;36m[libx264 @ 0x6f2620] [0musing SAR=1/1
    [1;36m[libx264 @ 0x6f2620] [0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 AVX2 LZCNT BMI2
    [1;36m[libx264 @ 0x6f2620] [0mprofile High, level 4.0
    [1;36m[libx264 @ 0x6f2620] [0m264 - core 148 r2643 5c65704 - H.264/MPEG-4 AVC codec - Copyleft 2003-2015 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=34 lookahead_threads=5 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00
    Output #0, mp4, to 'video.mp4':
      Metadata:
        encoder         : Lavf56.40.101
        Stream #0:0: Video: h264 (libx264) ([33][0][0][0] / 0x0021), yuvj420p(pc), 1920x1080 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc
        Metadata:
          encoder         : Lavc56.60.100 libx264
    Stream mapping:
      Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))
    Press [q] to stop, [?] for help
    frame=  449 fps= 24 q=-1.0 Lsize=    6326kB time=00:00:14.90 bitrate=3478.1kbits/s dup=75 drop=0    
    video:6321kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.086368%
    [1;36m[libx264 @ 0x6f2620] [0mframe I:4     Avg QP:21.71  size: 67667
    [1;36m[libx264 @ 0x6f2620] [0mframe P:237   Avg QP:22.70  size: 22238
    [1;36m[libx264 @ 0x6f2620] [0mframe B:208   Avg QP:24.44  size:  4473
    [1;36m[libx264 @ 0x6f2620] [0mconsecutive B-frames: 31.8% 13.4% 17.4% 37.4%
    [1;36m[libx264 @ 0x6f2620] [0mmb I  I16..4: 18.5% 72.3%  9.3%
    [1;36m[libx264 @ 0x6f2620] [0mmb P  I16..4:  4.1% 10.1%  0.9%  P16..4: 26.2%  8.9%  3.6%  0.0%  0.0%    skip:46.1%
    [1;36m[libx264 @ 0x6f2620] [0mmb B  I16..4:  0.4%  0.6%  0.1%  B16..8: 23.9%  1.8%  0.2%  direct: 0.7%  skip:72.4%  L0:36.0% L1:61.2% BI: 2.8%
    [1;36m[libx264 @ 0x6f2620] [0m8x8 transform intra:66.7% inter:77.8%
    [1;36m[libx264 @ 0x6f2620] [0mcoded y,uvDC,uvAC intra: 37.8% 47.6% 8.0% inter: 7.3% 7.2% 0.4%
    [1;36m[libx264 @ 0x6f2620] [0mi16 v,h,dc,p: 36% 34% 10% 20%
    [1;36m[libx264 @ 0x6f2620] [0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31% 25% 23%  3%  3%  4%  4%  4%  4%
    [1;36m[libx264 @ 0x6f2620] [0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 36% 25% 11%  4%  5%  6%  5%  4%  4%
    [1;36m[libx264 @ 0x6f2620] [0mi8c dc,h,v,p: 51% 23% 22%  3%
    [1;36m[libx264 @ 0x6f2620] [0mWeighted P-Frames: Y:3.0% UV:0.8%
    [1;36m[libx264 @ 0x6f2620] [0mref P L0: 72.7% 11.5% 10.7%  5.0%  0.1%
    [1;36m[libx264 @ 0x6f2620] [0mref B L0: 88.3%  9.9%  1.8%
    [1;36m[libx264 @ 0x6f2620] [0mref B L1: 93.9%  6.1%
    [1;36m[libx264 @ 0x6f2620] [0mkb/s:3459.21



```python
import IPython
IPython.display.Video('video.mp4')
```




<video src="video.mp4" controls>
      Your browser does not support the <code>video</code> element.
    </video>



# HubServingè½»é‡çº§æœåŠ¡åŒ–éƒ¨ç½²
**æ³¨æ„ï¼šä½¿ç”¨æ­¤æ–¹å¼éƒ¨ç½²ï¼Œéœ€ç¡®ä¿è‡ªå·±Pythonç¯å¢ƒä¸­PaddleHubçš„ç‰ˆæœ¬é«˜äº1.8.0,å› æ­¤éœ€è¦å°†AI Studioä¸­çš„Paddlehubå‡çº§ã€‚**
- [è½»é‡çº§æœåŠ¡åŒ–éƒ¨ç½²](https://github.com/paddlepaddle/PaddleX/blob/develop/docs/deploy/hub_serving.md)
- [PaddleHub-Serving](https://github.com/PaddlePaddle/PaddleHub/blob/develop/docs/tutorial/serving.md)
## éƒ¨ç½²æ¨¡å‹å‡†å¤‡

éƒ¨ç½²æ¨¡å‹çš„æ ¼å¼å‡ä¸ºç›®å½•ä¸‹åŒ…å«`__model__`ï¼Œ`__params__`å’Œ`model.yml`ä¸‰ä¸ªæ–‡ä»¶ï¼Œä¹Ÿå°±æ˜¯`inference_model`ç›®å½•ä¸‹çš„æ–‡ä»¶æ ¼å¼ã€‚


```python
!pip install paddlehub -U
```

## æ¨¡å‹è½¬æ¢

å°†`PaddleX`çš„`Inference Model`è½¬æ¢æˆ`PaddleHub`çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨å‘½ä»¤`hub convert`å³å¯ä¸€é”®è½¬æ¢ï¼Œå¯¹æ­¤å‘½ä»¤çš„è¯´æ˜å¦‚ä¸‹ï¼š
|å‚æ•°	|ç”¨é€”|
| -------- | -------- |
|--model_dir/-m |`PaddleX Inference Model`æ‰€åœ¨çš„ç›®å½•|
|--module_name/-n |ç”Ÿæˆé¢„è®­ç»ƒæ¨¡å‹çš„åç§°|
|--module_version/-v |ç”Ÿæˆé¢„è®­ç»ƒæ¨¡å‹çš„ç‰ˆæœ¬ï¼Œé»˜è®¤ä¸º`1.0.0`|
|--output_dir/-o |ç”Ÿæˆé¢„è®­ç»ƒæ¨¡å‹çš„å­˜æ”¾ä½ç½®ï¼Œé»˜è®¤ä¸º`{module_name}_{timestamp}`|


```python
!hub convert --model_dir inference_model \
              --module_name hatdet \
              --module_version 1.0 
```

    The converted module is stored in `hatdet_1598885245.2985997`.


## æ¨¡å‹å®‰è£…

å°†æ¨¡å‹è½¬æ¢å¾—åˆ°çš„`.tar.gz`æ ¼å¼çš„é¢„è®­ç»ƒæ¨¡å‹å‹ç¼©åŒ…ï¼Œåœ¨è¿›è¡Œéƒ¨ç½²ä¹‹å‰éœ€è¦å…ˆå®‰è£…åˆ°æœ¬æœºï¼Œä½¿ç”¨å‘½ä»¤`hub install`ä¸€é”®å®‰è£…


```python
!hub install hatdet_1598885245.2985997/hatdet.tar.gz
```

    Successfully installed hatdet


## æ¨¡å‹éƒ¨ç½²

æ‰“å¼€ç»ˆç«¯1ï¼Œè¾“å…¥`hub serving start -m hatdet`å®Œæˆå®‰å…¨å¸½æ£€æµ‹æ¨¡å‹çš„ä¸€é”®éƒ¨ç½²
![file](https://ai-studio-static-online.cdn.bcebos.com/e9bf1bb0bb3d4cd8a9df02d0aac6238f213ac39240d547a692ec2d6499306416)

## é¢„æµ‹ç»“æœä¸åå¤„ç†æ•ˆæœå±•ç¤º


```python
# coding: utf8
%matplotlib inline
import requests
import json
import cv2
import base64
import numpy as np
import colorsys
import warnings
warnings.filterwarnings("ignore")
plt.figure(figsize=(12,8))

def cv2_to_base64(image):
    data = cv2.imencode('.png', image)[1]
    return base64.b64encode(data.tostring()).decode('utf8')


if __name__ == '__main__':
    # è·å–å›¾ç‰‡çš„base64ç¼–ç æ ¼å¼
    img1 = cv2_to_base64(cv2.imread("MyDataset/images/hard_hat_workers1957.png"))
    img2 = cv2_to_base64(cv2.imread("MyDataset/images/hard_hat_workers1457.png"))
    data = {'images': [img1, img2]}
    # data = {'images': [img1]}
    # æŒ‡å®šcontent-type
    headers = {"Content-type": "application/json"}
    # å‘é€HTTPè¯·æ±‚
    url = "http://127.0.0.1:8866/predict/hatdet"
    r = requests.post(url=url, headers=headers, data=json.dumps(data))

    # æ‰“å°é¢„æµ‹ç»“æœï¼Œæ³¨æ„ï¼Œr.json()["results"]æœ¬èº«å°±æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œè¦å–åˆ°å¯¹åº”å›¾ç‰‡çš„é¢„æµ‹ç»“æœï¼Œéœ€æŒ‡å®šå…ƒç´ ä½ç½®ï¼Œå¦‚r.json()["results"][0]
    print(r.json()["results"])
    # ä½¿ç”¨é‡å†™çš„visualize()æ–¹æ³•å®Œæˆé¢„æµ‹ç»“æœåå¤„ç†
    # æ˜¾ç¤ºç¬¬ä¸€å¼ å›¾ç‰‡çš„é¢„æµ‹æ•ˆæœ
    image = visualize(cv2.imread('MyDataset/images/hard_hat_workers1957.png'),r.json()["results"][0], save_dir=None)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    plt.imshow(image)
    plt.axis('off') # ä¸æ˜¾ç¤ºåæ ‡è½´
    plt.show()
```

    [[{'bbox': [330.64300537109375, 229.9241943359375, 35.1065673828125, 33.229522705078125], 'category': 'helmet', 'category_id': 1, 'score': 0.9941794276237488}, {'bbox': [168.823974609375, 184.2073211669922, 54.227630615234375, 63.723907470703125], 'category': 'helmet', 'category_id': 1, 'score': 0.9892396330833435}, {'bbox': [367.8026123046875, 238.3224639892578, 46.005615234375, 45.48057556152344], 'category': 'helmet', 'category_id': 1, 'score': 0.9855232238769531}, {'bbox': [269.3492736816406, 220.7162322998047, 12.42694091796875, 15.184814453125], 'category': 'helmet', 'category_id': 1, 'score': 0.7675696015357971}, {'bbox': [247.13021850585938, 215.6059112548828, 13.21417236328125, 16.321746826171875], 'category': 'helmet', 'category_id': 1, 'score': 0.5700928568840027}, {'bbox': [232.14540100097656, 250.86013793945312, 10.022308349609375, 11.1153564453125], 'category': 'helmet', 'category_id': 1, 'score': 0.14765863120555878}, {'bbox': [362.0443420410156, 397.7749328613281, 52.1341552734375, 17.6563720703125], 'category': 'helmet', 'category_id': 1, 'score': 0.12965646386146545}, {'bbox': [258.4300537109375, 208.1387481689453, 24.2467041015625, 29.95611572265625], 'category': 'helmet', 'category_id': 1, 'score': 0.01179392822086811}, {'bbox': [320.583740234375, 385.3279113769531, 47.884765625, 30.672088623046875], 'category': 'helmet', 'category_id': 1, 'score': 0.011060410179197788}, {'bbox': [365.4349060058594, 263.9226989746094, 13.45458984375, 15.71697998046875], 'category': 'helmet', 'category_id': 1, 'score': 0.010737976059317589}, {'bbox': [189.65455627441406, 232.5000762939453, 22.3367919921875, 22.47003173828125], 'category': 'helmet', 'category_id': 1, 'score': 0.010102566331624985}, {'bbox': [320.981201171875, 357.64654541015625, 81.0762939453125, 55.97418212890625], 'category': 'person', 'category_id': 2, 'score': 0.020061025395989418}], [{'bbox': [131.40003967285156, 2.1436729431152344, 114.73101806640625, 107.99861526489258], 'category': 'helmet', 'category_id': 1, 'score': 0.9960896372795105}]]



![png](../imgs/output_38_1.png)

